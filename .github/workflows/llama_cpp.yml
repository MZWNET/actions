name: Build llama.cpp

on:
  workflow_dispatch:
    inputs:
      version:
        description: "Release version"
        required: true
        type: string

permissions:
  contents: write

jobs:
  build_intel_mkl_avx2_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir build
          cd build
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=Intel10_64_dyn -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_NATIVE=ON -DLLAMA_AVX2=ON
          cmake --build . --config Release
      - name: Pack
        run: |
          mkdir -p build/dist 
          cd build/bin/
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx2-intel-mkl-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: intel-mkl-avx2-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx2-intel-mkl-x64.zip
  build_cu121_avx2_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA 12.1
        id: install-cuda
        uses: Jimver/cuda-toolkit@v0.2.13
        with:
          cuda: "12.1.1"
      - name: Install Deps
        run: sudo apt-get install ccache ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUBLAS=ON -DLLAMA_AVX2=ON
          cmake --build . --config Release
      - name: Pack
        run: |
          mkdir -p build/dist
          cd build/bin/
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx2-cublas-cu121-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: cu121-avx2-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx2-cublas-cu121-x64.zip
  release:
    runs-on: ubuntu-latest
    needs: [build_intel_mkl_avx2_linux, build_cu121_avx2_linux]
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v3
        with:
          path: |
            dist/
      - name: Prepare Release
        run: |
          cd dist
          mv -f intel-mkl-avx2-linux/* .
          mv -f cu121-avx2-linux/* .
          rm -rf intel-mkl-avx2-linux cu121-avx2-linux
      - name: Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: llama_cpp-${{ inputs.version }}
          name: Binary build for llama.cpp (${{ inputs.version }})
          body: |
            This release contains the x86_64 binary of [llama.cpp](https://github.com/ggerganov/llama.cpp).
          files: dist/*
          fail_on_unmatched_files: true

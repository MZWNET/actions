name: Build llama.cpp

on:
  workflow_dispatch:
    inputs:
      version:
        description: "Release version"
        required: true
        type: string

permissions:
  contents: write

jobs:
  build_cu126_avx512_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA 12.6.1
        id: install-cuda
        uses: MZWNET/cuda-toolkit@mod
        with:
          cuda: "12.6.1"
          method: network
      - name: Install Deps
        run: sudo apt-get install ccache ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir -p build
          cd build || exit
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DGGML_CUDA_F16=true -DGGML_AVX512=ON -DGGML_CUDA_FA_ALL_QUANTS=ON
          cmake --build . --config Release -j "$(nproc)"
      - name: Pack
        run: |
          mkdir -p build/dist
          cd bin/ || exit
          mv ../*.so ./
          mv ../build/examples/**/*.so ./
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx512-cuda-cu12.6.1-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cu126-avx512-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx512-cuda-cu12.6.1-x64.zip
  release:
    runs-on: ubuntu-latest
    needs: [build_cu126_avx512_linux]
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: |
            dist/
      - name: Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: llama_cpp-${{ inputs.version }}
          name: Binary build for llama.cpp (${{ inputs.version }})
          body: |
            This release contains the x86_64 binary of [llama.cpp](https://github.com/ggerganov/llama.cpp).
          files: dist/**/*
          fail_on_unmatched_files: true

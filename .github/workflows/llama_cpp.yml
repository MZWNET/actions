name: Build llama.cpp

on:
  workflow_dispatch:
    inputs:
      version:
        description: "Release version"
        required: true
        type: string

permissions:
  contents: write

jobs:
  build_intel_mkl_avx2_linux:
    runs-on: ubuntu-20.04
    steps:
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir -p build
          cd build || exit
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_NATIVE=ON -DGGML_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist 
          cd build/bin/
          find . -name "*.so" -exec mv {} build/bin/ \;
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx2-intel-mkl-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: intel-mkl-avx2-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx2-intel-mkl-x64.zip
  build_sycl_avx512_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA
        id: install-cuda
        uses: MZWNET/cuda-toolkit@mod
        with:
          cuda: "12.6.2"
          method: network
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          wget -qO - https://developer.codeplay.com/apt/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/codeplay-keyring.gpg > /dev/null && echo "deb [signed-by=/usr/share/keyrings/codeplay-keyring.gpg] https://developer.codeplay.com/apt all main" | sudo tee /etc/apt/sources.list.d/codeplay.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit oneapi-nvidia-12.0 ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir -p build
          cd build || exit
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON -DGGML_SYCL_TARGET=NVIDIA -DGGML_SYCL_DEVICE_ARCH=sm_86 -DGGML_AVX512=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist 
          cd build/bin/
          find . -name "*.so" -exec mv {} build/bin/ \;
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx512-sycl-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sycl-avx512-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx512-sycl-x64.zip
  build_sycl_avx2_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install Deps
        run: |
          wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
          echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
          sudo apt-get update
          sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir -p build
          cd build || exit
          source /opt/intel/oneapi/setvars.sh
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON -DGGML_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist 
          cd build/bin/
          find . -name "*.so" -exec mv {} build/bin/ \;
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx2-sycl-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sycl-avx2-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx2-sycl-x64.zip
  build_cu126_avx512_linux:
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA
        id: install-cuda
        uses: MZWNET/cuda-toolkit@mod
        with:
          cuda: "12.6.2"
          method: network
      - name: Install Deps
        run: sudo apt-get install ccache ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          submodules: recursive
          ref: ${{ inputs.version }}
      - name: Build
        run: |
          mkdir -p build
          cd build || exit
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DGGML_AVX512=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DCMAKE_CUDA_ARCHITECTURES=86
          cmake --build . --config Release -j "$(nproc)"
      - name: Pack
        run: |
          mkdir -p build/dist
          find . -name "*.so" -exec mv {} build/bin/ \;
          cd build/bin/ || exit
          zip ../dist/llama-${{ inputs.version }}-bin-linux-avx512-cuda-cu12.6.2-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cu126-avx512-linux
          path: build/dist/llama-${{ inputs.version }}-bin-linux-avx512-cuda-cu12.6.2-x64.zip
  release:
    runs-on: ubuntu-latest
    needs: [build_intel_mkl_avx2_linux, build_sycl_avx512_linux, build_sycl_avx2_linux, build_cu126_avx512_linux]
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: |
            dist/
      - name: Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: llama_cpp-${{ inputs.version }}
          name: Binary build for llama.cpp (${{ inputs.version }})
          body: |
            This release contains the x86_64 binary of [llama.cpp](https://github.com/ggerganov/llama.cpp).
          files: dist/**/*
          fail_on_unmatched_files: true
